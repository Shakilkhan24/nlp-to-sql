paths:
  train_data: MyTrials/train.json
  checkpoint: transformer_model.pt

data:
  tokenizer_name: t5-small
  max_len: 128

model:
  d_model: 256
  num_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  d_ff: 1024        # originial paper 2048 
  max_len: 128
  pad_idx: 0
  ignore_index: -100

training:
  batch_size: 32
  epochs: 15
  learning_rate: 0.0005
  grad_clip_norm: 0.5
  warmup_steps: 1000
  label_smoothing: 0.1
  
inference:
  tokenizer_name: t5-small
  max_decode_len: 50